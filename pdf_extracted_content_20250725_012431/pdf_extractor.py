import os
import sys
import json
import pandas as pd
import pdfplumber
import PyPDF2
from pdf2image import convert_from_path
from PIL import Image
import tabula
import camelot
from pdfminer.high_level import extract_text
from pdfminer.layout import LAParams
import matplotlib.pyplot as plt
import cv2
import numpy as np
from datetime import datetime
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class ComprehensivePDFExtractor:
    def __init__(self, pdf_path, output_dir):
        self.pdf_path = pdf_path
        self.output_dir = output_dir
        self.metadata = {
            'extraction_time': datetime.now().isoformat(),
            'pdf_file': os.path.basename(pdf_path),
            'total_pages': 0,
            'extracted_items': {
                'text_files': [],
                'images': [],
                'tables': [],
                'charts': []
            }
        }
        
    def extract_metadata(self):
        """æå–PDFå…ƒæ•°æ®"""
        try:
            with open(self.pdf_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                self.metadata['total_pages'] = len(pdf_reader.pages)
                
                if pdf_reader.metadata:
                    self.metadata['pdf_metadata'] = {
                        'title': pdf_reader.metadata.get('/Title', ''),
                        'author': pdf_reader.metadata.get('/Author', ''),
                        'subject': pdf_reader.metadata.get('/Subject', ''),
                        'creator': pdf_reader.metadata.get('/Creator', ''),
                        'producer': pdf_reader.metadata.get('/Producer', ''),
                        'creation_date': str(pdf_reader.metadata.get('/CreationDate', '')),
                        'modification_date': str(pdf_reader.metadata.get('/ModDate', ''))
                    }
                    
            logger.info(f"PDFå…ƒæ•°æ®æå–å®Œæˆï¼Œæ€»é¡µæ•°: {self.metadata['total_pages']}")
            
        except Exception as e:
            logger.error(f"æå–PDFå…ƒæ•°æ®å¤±è´¥: {e}")
            
    def extract_text_content(self):
        """æå–æ–‡æœ¬å†…å®¹"""
        logger.info("å¼€å§‹æå–æ–‡æœ¬å†…å®¹...")
        
        try:
            # æ–¹æ³•1: ä½¿ç”¨pdfplumberæå–ç»“æ„åŒ–æ–‡æœ¬
            with pdfplumber.open(self.pdf_path) as pdf:
                full_text = ""
                page_texts = {}
                
                for i, page in enumerate(pdf.pages, 1):
                    page_text = page.extract_text()
                    if page_text:
                        page_texts[f"page_{i}"] = page_text
                        full_text += f"\n\n=== ç¬¬ {i} é¡µ ===\n\n{page_text}"
                        
                        # ä¿å­˜å•é¡µæ–‡æœ¬
                        page_file = os.path.join(self.output_dir, 'text', f'page_{i:03d}.txt')
                        with open(page_file, 'w', encoding='utf-8') as f:
                            f.write(page_text)
                        self.metadata['extracted_items']['text_files'].append(f'page_{i:03d}.txt')
                
                # ä¿å­˜å®Œæ•´æ–‡æœ¬
                full_text_file = os.path.join(self.output_dir, 'text', 'full_text.txt')
                with open(full_text_file, 'w', encoding='utf-8') as f:
                    f.write(full_text)
                    
                # ä¿å­˜ç»“æ„åŒ–æ–‡æœ¬æ•°æ®
                structured_file = os.path.join(self.output_dir, 'text', 'structured_text.json')
                with open(structured_file, 'w', encoding='utf-8') as f:
                    json.dump(page_texts, f, ensure_ascii=False, indent=2)
                    
                self.metadata['extracted_items']['text_files'].extend(['full_text.txt', 'structured_text.json'])
                
            # æ–¹æ³•2: ä½¿ç”¨pdfmineræå–æ›´è¯¦ç»†çš„æ–‡æœ¬
            detailed_text = extract_text(self.pdf_path, laparams=LAParams())
            detailed_file = os.path.join(self.output_dir, 'text', 'detailed_text.txt')
            with open(detailed_file, 'w', encoding='utf-8') as f:
                f.write(detailed_text)
            self.metadata['extracted_items']['text_files'].append('detailed_text.txt')
            
            logger.info(f"æ–‡æœ¬æå–å®Œæˆï¼Œå…±æå– {len(page_texts)} é¡µ")
            
        except Exception as e:
            logger.error(f"æ–‡æœ¬æå–å¤±è´¥: {e}")
            
    def extract_images(self):
        """æå–å›¾ç‰‡å†…å®¹"""
        logger.info("å¼€å§‹æå–å›¾ç‰‡...")
        
        try:
            # æ–¹æ³•1: è½¬æ¢æ•´é¡µä¸ºå›¾ç‰‡
            pages = convert_from_path(self.pdf_path, dpi=300)
            for i, page in enumerate(pages, 1):
                image_file = os.path.join(self.output_dir, 'images', f'page_{i:03d}_full.png')
                page.save(image_file, 'PNG')
                self.metadata['extracted_items']['images'].append(f'page_{i:03d}_full.png')
                
            # æ–¹æ³•2: æå–åµŒå…¥çš„å›¾ç‰‡
            with pdfplumber.open(self.pdf_path) as pdf:
                for i, page in enumerate(pdf.pages, 1):
                    # æ£€æµ‹å›¾ç‰‡å¯¹è±¡
                    if hasattr(page, 'images'):
                        for j, img in enumerate(page.images):
                            try:
                                # æå–å›¾ç‰‡åŒºåŸŸ
                                bbox = (img['x0'], img['top'], img['x1'], img['bottom'])
                                cropped = page.crop(bbox)
                                
                                # è½¬æ¢ä¸ºå›¾ç‰‡
                                img_obj = cropped.to_image(resolution=300)
                                img_file = os.path.join(self.output_dir, 'images', f'page_{i:03d}_img_{j+1}.png')
                                img_obj.save(img_file)
                                self.metadata['extracted_items']['images'].append(f'page_{i:03d}_img_{j+1}.png')
                                
                            except Exception as e:
                                logger.warning(f"æå–ç¬¬{i}é¡µç¬¬{j+1}ä¸ªå›¾ç‰‡å¤±è´¥: {e}")
                                
            logger.info(f"å›¾ç‰‡æå–å®Œæˆï¼Œå…±æå– {len(self.metadata['extracted_items']['images'])} ä¸ªå›¾ç‰‡")
            
        except Exception as e:
            logger.error(f"å›¾ç‰‡æå–å¤±è´¥: {e}")
            
    def extract_tables(self):
        """æå–è¡¨æ ¼å†…å®¹"""
        logger.info("å¼€å§‹æå–è¡¨æ ¼...")
        
        try:
            # æ–¹æ³•1: ä½¿ç”¨tabula-pyæå–è¡¨æ ¼
            try:
                tables = tabula.read_pdf(self.pdf_path, pages='all', multiple_tables=True)
                for i, table in enumerate(tables):
                    if not table.empty:
                        table_file = os.path.join(self.output_dir, 'tables', f'tabula_table_{i+1:03d}.csv')
                        table.to_csv(table_file, index=False, encoding='utf-8')
                        
                        # åŒæ—¶ä¿å­˜ä¸ºExcel
                        excel_file = os.path.join(self.output_dir, 'tables', f'tabula_table_{i+1:03d}.xlsx')
                        table.to_excel(excel_file, index=False)
                        
                        self.metadata['extracted_items']['tables'].extend([
                            f'tabula_table_{i+1:03d}.csv',
                            f'tabula_table_{i+1:03d}.xlsx'
                        ])
                        
                logger.info(f"Tabulaæå–äº† {len(tables)} ä¸ªè¡¨æ ¼")
                
            except Exception as e:
                logger.warning(f"Tabulaè¡¨æ ¼æå–å¤±è´¥: {e}")
                
            # æ–¹æ³•2: ä½¿ç”¨camelotæå–è¡¨æ ¼
            try:
                tables = camelot.read_pdf(self.pdf_path, pages='all')
                for i, table in enumerate(tables):
                    table_file = os.path.join(self.output_dir, 'tables', f'camelot_table_{i+1:03d}.csv')
                    table.to_csv(table_file, encoding='utf-8')
                    
                    # ä¿å­˜è¡¨æ ¼å›¾ç‰‡
                    table_img = os.path.join(self.output_dir, 'tables', f'camelot_table_{i+1:03d}_visual.png')
                    camelot.plot(table, kind='contour').savefig(table_img)
                    plt.close()
                    
                    self.metadata['extracted_items']['tables'].extend([
                        f'camelot_table_{i+1:03d}.csv',
                        f'camelot_table_{i+1:03d}_visual.png'
                    ])
                    
                logger.info(f"Camelotæå–äº† {len(tables)} ä¸ªè¡¨æ ¼")
                
            except Exception as e:
                logger.warning(f"Camelotè¡¨æ ¼æå–å¤±è´¥: {e}")
                
            # æ–¹æ³•3: ä½¿ç”¨pdfplumberæå–è¡¨æ ¼
            try:
                with pdfplumber.open(self.pdf_path) as pdf:
                    table_count = 0
                    for i, page in enumerate(pdf.pages, 1):
                        tables = page.extract_tables()
                        for j, table in enumerate(tables):
                            if table:
                                table_count += 1
                                df = pd.DataFrame(table[1:], columns=table[0])
                                
                                table_file = os.path.join(self.output_dir, 'tables', f'pdfplumber_page_{i:03d}_table_{j+1}.csv')
                                df.to_csv(table_file, index=False, encoding='utf-8')
                                
                                excel_file = os.path.join(self.output_dir, 'tables', f'pdfplumber_page_{i:03d}_table_{j+1}.xlsx')
                                df.to_excel(excel_file, index=False)
                                
                                self.metadata['extracted_items']['tables'].extend([
                                    f'pdfplumber_page_{i:03d}_table_{j+1}.csv',
                                    f'pdfplumber_page_{i:03d}_table_{j+1}.xlsx'
                                ])
                                
                logger.info(f"PDFPlumberæå–äº† {table_count} ä¸ªè¡¨æ ¼")
                
            except Exception as e:
                logger.warning(f"PDFPlumberè¡¨æ ¼æå–å¤±è´¥: {e}")
                
        except Exception as e:
            logger.error(f"è¡¨æ ¼æå–å¤±è´¥: {e}")
            
    def detect_charts(self):
        """æ£€æµ‹å’Œæå–å›¾è¡¨"""
        logger.info("å¼€å§‹æ£€æµ‹å›¾è¡¨...")
        
        try:
            # è½¬æ¢PDFé¡µé¢ä¸ºå›¾ç‰‡è¿›è¡Œå›¾è¡¨æ£€æµ‹
            pages = convert_from_path(self.pdf_path, dpi=200)
            
            for i, page in enumerate(pages, 1):
                # è½¬æ¢ä¸ºOpenCVæ ¼å¼
                page_np = np.array(page)
                page_cv = cv2.cvtColor(page_np, cv2.COLOR_RGB2BGR)
                gray = cv2.cvtColor(page_cv, cv2.COLOR_BGR2GRAY)
                
                # æ£€æµ‹å›¾è¡¨ç‰¹å¾ï¼ˆç®€å•çš„çŸ©å½¢æ£€æµ‹ï¼‰
                edges = cv2.Canny(gray, 50, 150, apertureSize=3)
                contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                
                chart_count = 0
                for j, contour in enumerate(contours):
                    area = cv2.contourArea(contour)
                    if area > 5000:  # è¿‡æ»¤å°çš„å™ªå£°
                        x, y, w, h = cv2.boundingRect(contour)
                        aspect_ratio = w / h
                        
                        # ç®€å•çš„å›¾è¡¨åˆ¤æ–­é€»è¾‘
                        if 0.5 < aspect_ratio < 3 and w > 100 and h > 100:
                            chart_count += 1
                            
                            # æå–å›¾è¡¨åŒºåŸŸ
                            chart_region = page_cv[y:y+h, x:x+w]
                            chart_file = os.path.join(self.output_dir, 'charts', f'page_{i:03d}_chart_{chart_count}.png')
                            cv2.imwrite(chart_file, chart_region)
                            
                            self.metadata['extracted_items']['charts'].append(f'page_{i:03d}_chart_{chart_count}.png')
                            
                            # åœ¨åŸå›¾ä¸Šæ ‡è®°å›¾è¡¨ä½ç½®
                            cv2.rectangle(page_cv, (x, y), (x+w, y+h), (0, 255, 0), 2)
                            cv2.putText(page_cv, f'Chart {chart_count}', (x, y-10), 
                                      cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
                
                # ä¿å­˜æ ‡è®°äº†å›¾è¡¨çš„é¡µé¢
                if chart_count > 0:
                    marked_file = os.path.join(self.output_dir, 'charts', f'page_{i:03d}_marked.png')
                    cv2.imwrite(marked_file, page_cv)
                    self.metadata['extracted_items']['charts'].append(f'page_{i:03d}_marked.png')
                    
            logger.info(f"å›¾è¡¨æ£€æµ‹å®Œæˆï¼Œå…±æ£€æµ‹åˆ° {len([f for f in self.metadata['extracted_items']['charts'] if 'chart_' in f])} ä¸ªå›¾è¡¨")
            
        except Exception as e:
            logger.error(f"å›¾è¡¨æ£€æµ‹å¤±è´¥: {e}")
            
    def generate_analysis_report(self):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        logger.info("ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
        
        try:
            report = {
                'extraction_summary': {
                    'pdf_file': self.metadata['pdf_file'],
                    'extraction_time': self.metadata['extraction_time'],
                    'total_pages': self.metadata['total_pages'],
                    'text_files_count': len(self.metadata['extracted_items']['text_files']),
                    'images_count': len(self.metadata['extracted_items']['images']),
                    'tables_count': len(self.metadata['extracted_items']['tables']),
                    'charts_count': len(self.metadata['extracted_items']['charts'])
                },
                'detailed_inventory': self.metadata['extracted_items'],
                'recommendations': []
            }
            
            # æ·»åŠ å»ºè®®
            if report['extraction_summary']['text_files_count'] > 0:
                report['recommendations'].append("æ–‡æœ¬å†…å®¹å·²æˆåŠŸæå–ï¼Œå¯ç”¨äºå†…å®¹åˆ†æå’Œæœç´¢")
                
            if report['extraction_summary']['tables_count'] > 0:
                report['recommendations'].append("è¡¨æ ¼æ•°æ®å·²æå–ä¸ºCSVå’ŒExcelæ ¼å¼ï¼Œå¯è¿›è¡Œæ•°æ®åˆ†æ")
                
            if report['extraction_summary']['images_count'] > 0:
                report['recommendations'].append("å›¾ç‰‡å·²æå–ä¸ºPNGæ ¼å¼ï¼Œå¯ç”¨äºè§†è§‰åˆ†æ")
                
            if report['extraction_summary']['charts_count'] > 0:
                report['recommendations'].append("å›¾è¡¨å·²æ£€æµ‹å¹¶æå–ï¼Œå¯ç”¨äºæ•°æ®å¯è§†åŒ–åˆ†æ")
            
            # ä¿å­˜æŠ¥å‘Š
            report_file = os.path.join(self.output_dir, 'reports', 'extraction_report.json')
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
                
            # ç”ŸæˆHTMLæŠ¥å‘Š
            html_report = self.generate_html_report(report)
            html_file = os.path.join(self.output_dir, 'reports', 'extraction_report.html')
            with open(html_file, 'w', encoding='utf-8') as f:
                f.write(html_report)
                
            logger.info("åˆ†ææŠ¥å‘Šç”Ÿæˆå®Œæˆ")
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆåˆ†ææŠ¥å‘Šå¤±è´¥: {e}")
            
    def generate_html_report(self, report):
        """ç”ŸæˆHTMLæ ¼å¼çš„æŠ¥å‘Š"""
        html = f"""
        <!DOCTYPE html>
        <html lang="zh-CN">
        <head>
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>PDFå†…å®¹æå–æŠ¥å‘Š</title>
            <style>
                body {{ font-family: 'Microsoft YaHei', sans-serif; margin: 20px; }}
                .header {{ background: #f5f5f5; padding: 20px; border-radius: 8px; }}
                .section {{ margin: 20px 0; padding: 15px; border: 1px solid #ddd; border-radius: 5px; }}
                .summary-grid {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 15px; }}
                .summary-card {{ background: #e8f4fd; padding: 15px; border-radius: 5px; text-align: center; }}
                .file-list {{ max-height: 300px; overflow-y: auto; }}
                .file-item {{ padding: 5px; border-bottom: 1px solid #eee; }}
                .recommendation {{ background: #f0f8f0; padding: 10px; margin: 5px 0; border-left: 4px solid #4CAF50; }}
            </style>
        </head>
        <body>
            <div class="header">
                <h1>PDFå†…å®¹æå–æŠ¥å‘Š</h1>
                <p><strong>æ–‡ä»¶:</strong> {report['extraction_summary']['pdf_file']}</p>
                <p><strong>æå–æ—¶é—´:</strong> {report['extraction_summary']['extraction_time']}</p>
            </div>
            
            <div class="section">
                <h2>æå–æ¦‚è¦</h2>
                <div class="summary-grid">
                    <div class="summary-card">
                        <h3>{report['extraction_summary']['total_pages']}</h3>
                        <p>æ€»é¡µæ•°</p>
                    </div>
                    <div class="summary-card">
                        <h3>{report['extraction_summary']['text_files_count']}</h3>
                        <p>æ–‡æœ¬æ–‡ä»¶</p>
                    </div>
                    <div class="summary-card">
                        <h3>{report['extraction_summary']['images_count']}</h3>
                        <p>å›¾ç‰‡æ–‡ä»¶</p>
                    </div>
                    <div class="summary-card">
                        <h3>{report['extraction_summary']['tables_count']}</h3>
                        <p>è¡¨æ ¼æ–‡ä»¶</p>
                    </div>
                    <div class="summary-card">
                        <h3>{report['extraction_summary']['charts_count']}</h3>
                        <p>å›¾è¡¨æ–‡ä»¶</p>
                    </div>
                </div>
            </div>
            
            <div class="section">
                <h2>æ–‡ä»¶æ¸…å•</h2>
                <h3>æ–‡æœ¬æ–‡ä»¶</h3>
                <div class="file-list">
                    {''.join([f'<div class="file-item">ğŸ“„ {f}</div>' for f in report['detailed_inventory']['text_files']])}
                </div>
                
                <h3>å›¾ç‰‡æ–‡ä»¶</h3>
                <div class="file-list">
                    {''.join([f'<div class="file-item">ğŸ–¼ï¸ {f}</div>' for f in report['detailed_inventory']['images']])}
                </div>
                
                <h3>è¡¨æ ¼æ–‡ä»¶</h3>
                <div class="file-list">
                    {''.join([f'<div class="file-item">ğŸ“Š {f}</div>' for f in report['detailed_inventory']['tables']])}
                </div>
                
                <h3>å›¾è¡¨æ–‡ä»¶</h3>
                <div class="file-list">
                    {''.join([f'<div class="file-item">ğŸ“ˆ {f}</div>' for f in report['detailed_inventory']['charts']])}
                </div>
            </div>
            
            <div class="section">
                <h2>ä½¿ç”¨å»ºè®®</h2>
                {''.join([f'<div class="recommendation">ğŸ’¡ {rec}</div>' for rec in report['recommendations']])}
            </div>
        </body>
        </html>
        """
        return html
        
    def run_extraction(self, extract_text=True, extract_images=True, extract_tables=True, detect_charts=True):
        """è¿è¡Œå®Œæ•´çš„æå–æµç¨‹"""
        logger.info(f"å¼€å§‹å¤„ç†PDFæ–‡ä»¶: {self.pdf_path}")
        
        # æå–å…ƒæ•°æ®
        self.extract_metadata()
        
        # æ ¹æ®å‚æ•°é€‰æ‹©æå–å†…å®¹
        if extract_text:
            self.extract_text_content()
            
        if extract_images:
            self.extract_images()
            
        if extract_tables:
            self.extract_tables()
            
        if detect_charts:
            self.detect_charts()
            
        # ç”ŸæˆæŠ¥å‘Š
        self.generate_analysis_report()
        
        # ä¿å­˜å…ƒæ•°æ®
        metadata_file = os.path.join(self.output_dir, 'metadata', 'extraction_metadata.json')
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(self.metadata, f, ensure_ascii=False, indent=2)
            
        logger.info("PDFå†…å®¹æå–å®Œæˆ!")
        return self.metadata

def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='ç»¼åˆPDFå†…å®¹æå–å·¥å…·')
    parser.add_argument('pdf_path', help='PDFæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output-dir', default='pdf_extracted_content', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--no-text', action='store_true', help='ä¸æå–æ–‡æœ¬')
    parser.add_argument('--no-images', action='store_true', help='ä¸æå–å›¾ç‰‡')
    parser.add_argument('--no-tables', action='store_true', help='ä¸æå–è¡¨æ ¼')
    parser.add_argument('--no-charts', action='store_true', help='ä¸æ£€æµ‹å›¾è¡¨')
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)
    for subdir in ['text', 'images', 'tables', 'charts', 'metadata', 'reports']:
        os.makedirs(os.path.join(args.output_dir, subdir), exist_ok=True)
    
    # åˆ›å»ºæå–å™¨å¹¶è¿è¡Œ
    extractor = ComprehensivePDFExtractor(args.pdf_path, args.output_dir)
    result = extractor.run_extraction(
        extract_text=not args.no_text,
        extract_images=not args.no_images,
        extract_tables=not args.no_tables,
        detect_charts=not args.no_charts
    )
    
    print(f"\næå–å®Œæˆ! ç»“æœä¿å­˜åœ¨: {args.output_dir}")
    print(f"æ€»é¡µæ•°: {result['total_pages']}")
    print(f"æå–çš„æ–‡ä»¶æ•°é‡:")
    print(f"  - æ–‡æœ¬æ–‡ä»¶: {len(result['extracted_items']['text_files'])}")
    print(f"  - å›¾ç‰‡æ–‡ä»¶: {len(result['extracted_items']['images'])}")
    print(f"  - è¡¨æ ¼æ–‡ä»¶: {len(result['extracted_items']['tables'])}")
    print(f"  - å›¾è¡¨æ–‡ä»¶: {len(result['extracted_items']['charts'])}")

if __name__ == "__main__":
    main()
